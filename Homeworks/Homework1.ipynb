{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 1\n",
    "1) How would you define Machine Learning?\n",
    "\n",
    "Machine Learning (ML) is a sub-branch of Artifical Intelligence that allows the researchers for building statistical models to learn from the data and improve from the experience to get better results without being explicitly programmed. ML algorithms primarily focuses on the development of programs to allow computers learn automatically, mainly categorized under supervised and unsupervised learning titles.\n",
    "\n",
    "\n",
    "2) What are the differences between Supervised and Unsupervised Learning? Specify example 3 algorithms for each of these.\n",
    "\n",
    "In general, they both work on the different scenarios and with different datasets. In the case of supervised learning, algorithm learns from the labeled training data, meanwhile unsupervised learning algorithms deal with the unlabelled data. Equivalently, supervised learning needs supervision for the model training, while unsupervised learning allow the model to work on its own way to discover information from the data. In general, classification and regression problems are listed under supervised learning. On the other hand, clustering and association problems lie under the unsupervised learning. For each of them, the following 3 examples can be given. \n",
    "\n",
    "Supervised Learning;\n",
    "1. Linear Regression\n",
    "2. Logistic Regression\n",
    "3. Decision Trees\n",
    "\n",
    "Unsupervised Learning;\n",
    "1. Hierarchical clustering\n",
    "2. K-means clustering\n",
    "3. Principal Component Analysis\n",
    "\n",
    "3) What are the test and validation set, and why would you want to use them?\n",
    "After pre-processing the data set to make it ready for the building models, the data set is partitioned into subsets for different purposes. In general, 3 different data sets are created from the original set, called as; train, validation and test sets. The validation test is used in general for evaluating a given model, means that the model parameters are hyper-tuned by using this data set. Afterwards, test data set serves us for providing an unbiased evaluation of a final model. This data set is used only once after completing both train and validation sets, to evaluate the model. \n",
    "\n",
    "4) What are the main preprocessing steps? Explain them in detail. Why we need to prepare our data?\n",
    "Before splitting the data set, the practitioner should be aware of the readiness of the data set for model fitting. In general, these setps can be listed as follows;\n",
    "\n",
    "1. Removing duplicates: To avoid the repeated unwanted observation before running any algorithm\n",
    "2. Data Structure: Determining whether the data is balanced where the number of observations are equal for each class or imbalanced one, where the number of observations of a class(es) are significantly higher than another class(es).\n",
    "3. Missing value elimination: Any missing value belonging to any class lead to imbalanced data structure. To avoid this issue, one can remove it from the sample directly or alternatively the missing value can be filled by mean or median value based on the distribution of the data. \n",
    "4. Outlier Detection: Similar to the missing value, the extreme values from the sample can be eliminated if necessary. This detection can be made using box plots/IQR calculation\n",
    "5. Feature Scaling: One of the most important step for data pre-processing to get more reliable model outputs. For instance, objective functions do not work correctly without normalization in some ML algorihms, since the range of raw data varies widely. Under this title we have options as follows;\n",
    "\n",
    "a. Standardization\n",
    "Standardization is a technique where the values are centered around the mean with a unit standard deviation, this implies simply that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation. It can be done\n",
    "using the function, X_new = (X - E(X)) / sigma(X) where X is the original value, E(X) is the mean of X and sigma(X) is the standard deviation of X. \n",
    "\n",
    "b. Normalization \n",
    "Normalization or Min-Max scaling is another technique where the values are shifted and rescaled so that they lie between 0 and 1 at the end. It can be made by using the function, X_new = (X - min(X)) / (max(X)-min(X)) where max() is the maximum and min() is the minimum values of the feature X. \n",
    "\n",
    "c. Others\n",
    "Other alternatives might be the log transformation based on nature of the data set. Or Bucketing (Data binning) can be considered as another data pre-processing technique to minimize the impacts of small observation errors (noisy data). \n",
    "\n",
    "6. Feature Extraction\n",
    "To reduce the feature dimensionality, the most relevan features can be selected to reduce the dimension before ML model building. There are some widely used methods for that purpose, like Principle Components Analysis (PCA), Independent Component Analysis (ICA), Linear Discriminant Analysis (LDA), t-distributed Stochastic Neighbor Embedding (t-SNE) and so on in the literature. This is important for getting more accurately learned models by preventing the use of redundant data from the input data set. \n",
    "7. Feature Encoding\n",
    "This is simply a transformation method for creating convenient input data for the model building. It allows us to create new easily accepted input data for ML algorithms while still keeping the original meaning of the data. Mainly, we have nominal and ordinal options for that transformation where nominal keeps the original meaning and makes a 1-1 mapping using the permutation of values like one-hot encoding. Ordinal one is useful for the cases where the order-preserving change of values is required for the analysis. For instance, the one can encode Dry, Normal and Wet weather conditions as {0, 1, 2} or any other reasonable ordered values.\n",
    "\n",
    "In general, the reason why we need such pre-processing steps is about the cleaning and organizing it to make it more suitable for a building and training ML models. Generally, the real world data sets have various drawbacks like being incomplete, inconsistent, or inaccurate (contains errors or outliers) so that before starting to build ML models, we need to make the data set ready first by cleaning/organizing/reformatting. \n",
    "\n",
    "5) How you can explore countionus and discrete variables?\n",
    "Firstly, exploratory data analysis step allows us to distinguish between continuos and discrete variables. By definition, discrete random variable is the variable that its value is obtained by counting so that it is countable whereas continuos ones can take any value within a range. To illustrate, counting the number of goals during a soccer play is a discrete random variable whereas the measured precipitation amount at a station is continuous one. To explore them, basic plotting tolls will be beneficial. For instance, bar chart represents the frequency of the discrete data to visualize the probability of each value using the height of bar plots. On the other hand, histogram can be used to understand the distribution of continuous variable over pre defined bins. \n",
    "\n",
    "6) Analyse the plot given below. (What is the plot and variable type, check the distribution and make comment about how you can preproccess it.)\n",
    "The given figure is the histogram of petal width variable (cm unit) and the corresponding normal curve for the data set. The variable is continuous and the graphs visualize the clear bimodal distribution based on the added curve. For the pre-processing part, for instance for a clustering problem for three different species, one need to focus on the box plots for petal width, over for each species class to see the different distributions for the feature for each species to continue. Also, by looking at the steps given in the answer of 4, the data set should be prepared to build any kind of ML model.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
